<?xml version="1.0"?>
<article xmlns:r="http://www.r-project.org"
         xmlns:xi="http://www.w3.org/2003/XInclude"
	 xmlns:c="http://www.C.org"
	 xmlns:omg="http://www.omegahat.org">

<articleinfo>

<title>Compiling GPU Kernels in <r/>
<subtitle><omg:pkg>Rllvm</omg:pkg>,  <omg:pkg>Rnvvm</omg:pkg> and <omg:pkg>RCUDA</omg:pkg></subtitle>
</title>

<author><firstname>Duncan</firstname><surname>Temple Lang</surname>
  <affiliation><orgname>University of California at Davis</orgname>
               <orgdiv>Department of Statistics</orgdiv>
  </affiliation>
</author>
</articleinfo>


<para>
The idea here is to create a very, very simple kernel to run on a GPU.
We do this by creating individual instructions using
<omg:pkg>Rllvm</omg:pkg>.  When we have defined the routine, we use
the <lib>nvvm</lib> library via the <omg:pkg>Rnvvm</omg:pkg> package
to transform the <llvm/> IR code to PTX code. 
We can also generate the PTX code directly within <llvm/>.
We can then load this
PTX code into the <r/> session using the <omg:pkg>RCUDA</omg:pkg>
package and invoke the kernel.  This simple example illustrates all of
the steps we need to compile more complex <r/> code as GPU kernels
that we can then run directly from the <r/> session.
</para>


<note><para> <lib>nvvm</lib> is available in the current release
candidate version of the CUDA SDK, namely 5.5.
<omg:pkg>Rllvm</omg:pkg>, <omg:pkg>RCUDA</omg:pkg> and
<omg:pkg>Rnvvm</omg:pkg> are available from github and also the
Omegahat repository.
</para>
</note>


<section>
<title>Generating the <llvm/> IR code</title>

<para>
We start by creating the <llvm/> instructions
to define our kernel. 
The kernel we want to implement is intentionally 
very, very simple and corresponds to the CUDA code
<c:code><![CDATA[
void kern(int N, int *out)
{
   int idx = blockIdx.x * blockDim.x + threadIdx.x;
   if(idx < N)
     out[idx] = idx;
}
]]></c:code>
This takes an array of integer values and 
each thread sets its (idx) element to <c:var>idx</c:var>
and we end up with 0, 1, 2, ...., N - 1 in <c:var>out</c:var>.
</para>

<para>
We could create the IR code by hand or generate this <c/> code
and compile it. However, we want  to illustrate how to do this generally
and  be able to compile <r/>-like code to a kernel. For this, 
we use <omg:pkg>Rllvm</omg:pkg>.
We load that library and also  some simple utility functions
that will simplify our code generation and are generic
for <llvm/> <r:class>Function</r:class>s that we will convert
to PTX via <lib>nvvm</lib>.
<r:code>
library(Rllvm)
source("nvvmUtils.R")
</r:code>
</para>
<para>
We can now start to create our kernel.
We create a <r:class>Module</r:class>.
Since we know we are targetting PTX code and <lib>nvvm</lib>,
we call <r:func>ModuleForNVVM</r:func> to create an
enhanced <r:class>Module</r:class>:
<r:code>
m = ModuleForNVVM("ptx kernel")
</r:code>
This function sets the data layout string on the module and also
registers the special PTX register accessor routines so that we can
use them in our code.  These are the routines that correspond to
accessing the x, y, z components of <c:var>threadIdx</c:var>,
<c:var>blockIdx</c:var>, <c:var>gridIdx</c:var>,
<c:var>blockDim</c:var>, <c:var>gridDim</c:var>
</para>

<para>
With the module created, we define our new <r:func>Function</r:func>
which will become or GPU kernel.
<r:code>
fun = simpleFunction("kern", VoidType, n = Int32Type, out = Int32PtrType, mod = m)
ir = fun$ir
localVars = fun$vars
fun = fun$fun
</r:code>
We have used <r:func>simpleFunction</r:func>
in order to simplify creating the 
<r:class>IRBuilder</r:class>, the
initial <r:class>BasicBlock</r:class> and
also to create local variables corresponding to the parameters.
We could use <r:func>Function</r:func> directly.

</para>
<para>
In order to be able to use this routine as a GPU kernel,
we need to indicate that it is a kernel and not a device
or host routine. We do this by
adding metadata to the module that identifies this 
as a kernel. 
We do this with
<r:code>
  # declare that this is a PTX kernel
setMetadata(m, "nvvm.annotations", list(fun, "kernel", 1L))
</r:code>
We can define multiple kernels in the same module.
See <ulink url="http://llvm.org/docs/NVPTXUsage.html"/> for more information.

</para>
<para>
We can now focus on implementing the routine.
The first step is to create
<c:code>
   int idx = blockIdx.x * blockDim.x + threadIdx.x;
</c:code>
The idea is that we will compute the index for this thread
and put that in a local variable <c:var>idx</c:var>.
We create the right-hand side
<r:code>
blockId = ir$createCall(PTXRegisterRoutines[["llvm.nvvm.read.ptx.sreg.ctaid.x"]])
blockDim = ir$createCall(PTXRegisterRoutines[["llvm.nvvm.read.ptx.sreg.ntid.x"]])
mul = ir$binOp(Mul, blockId, blockDim)
threadId = ir$createCall(PTXRegisterRoutines[["llvm.nvvm.read.ptx.sreg.tid.x"]])
idx = ir$binOp(Add, mul, threadId)
</r:code>
The important aspect of this is that we are accessing
<c:var>threadIdx.x</c:var>, for example, via
the <r:var>PTXRegisterRoutines</r:var> and the oddly named elements
<r:el>llvm.nvvm.read.ptx.sreg.tid.x</r:el>.
When we build a compiler  for creating GPU kernels, 
we'll allow the code to use <r:expr>threadIdx$x</r:expr> and
map these expressions to calls to the corresponding register routine.

</para>
<para>
We can now create the local variable and initialize
it with the value of the right-hand side:
<r:code>
i = ir$createLocalVariable(Int32Type, "idx")
ir$createStore(idx, i)
</r:code>
Note that we are using the term idx in different ways here.
In the first expression, we are using it as the name 
in the <llvm/> code. In the second expression, 
we are referring to the <r/> variable assigned in the
previous block of code that contains the addition of the two terms.
</para>

<para>
Our next step is to check if the value of <c:var>idx</c:var>
is less than our parameter <c:arg>N</c:arg>.
To do this, we need to create different blocks and conditionally
branch to the appropriate block.
One block will assign the value to the appropriate element in  our array
and jump to the end.
The other block will simply exit the kernel routine.
We create these blocks and the condition branch with 
<r:code>
set = Block(fun, "set")
end = Block(fun, "return")

cond = ir$createICmp(ICMP_SLT, i, localVars$n)
ir$createCondBr(cond, set, end)
</r:code>
</para>

<para>
We now implement the block that assigns the
value to the array.
We ensure we are adding code to this block and then use
a GEP instruction to access the relevant element of the array.
<q>We need this assignment to be in the global address space (1), not local.
libNVVM takes care of this for us. We might be able to do this directly
without <lib>nvvm</lib>. But for now, <omg:pkg>Rnvvm</omg:pkg> does it</q>
<r:code>
ir$setInsertBlock(set)
gep = ir$createGEP(ir$createLoad(localVars$out), ir$createSExt(ir$createLoad(i), 64L))
ir$createStore(ir$createLoad(i), gep)
ir$createBr(end)
</r:code>
The final command branches to the final block.
We could have, alternatively, added an explicit return here.
</para>

<para>
We can finish the code by adding a simple return 
to the final block:
<r:code>
ir$setInsertBlock(end)
ir$createReturn()
</r:code>
</para>
<para>
It is always a good idea to verify that the code in the module
is valid:
<r:code>
verifyModule(m)
</r:code>
</para>

</section>
<section>
<title>Converting the IR code to PTX</title>

<para>
The next step in getting the code onto the GPU
is to convert the IR code to PTX code.
We can do this directly with <llvm/>.
(See <file>llvmPTXUtils.R</file> in this directory.)
However, <lib>nvvm</lib> does additional processing that
gets the code above to work.
We can use <lib>nvvm</lib> in <r/> via the
<omg:pkg>Rnvvm</omg:pkg> package.
We get the IR code as a string 
and fix it up to remove <llvm/>
attributes that <lib>nvvm</lib> doesn't currently understand.
(This may be due to  different versions of the <llvm/> IR format.)
To convert the code, we use <r:func>generatePTX</r:func>:
<r:code>
library(Rnvvm)
code = showModule(m, TRUE)
code = fixPTXCodeForNVVM(code)
ptx = generatePTX(code, isFile = FALSE)
</r:code>
The <r:func>generatePTX</r:func> function in the <omg:pkg>Rnvvm</omg:pkg>
is a high-level function that uses the lower-level
routines in the <lib>nvvm</lib> API.
The result is a string containing the entire PTX code corresponding
to our <llvm/> module. We now have what we need to load onto the GPU.
</para>
</section>
<section>
<title>Using the kernel on the GPU</title>
<para>
The final step is to load the PTX code and
invoke it from <r/>.
For this, we use the <omg:pkg>RCUDA</omg:pkg> package
and <r:func>cuModuleLoadDataEx</r:func>
<r:code>
library(RCUDA)
cuda.mod = cuModuleLoadDataEx(ptx)
</r:code>
</para>

<para>
We can now  invoke the kernel, passing it an array of integers
and the number of elements it contains.
We'll pass an <r/> vector  that has fewer  elements 
than the number of GPU threads we run.
This will exercise the condition in our code.
We'll run 32 x 32 threads. We'll pass a vector
with 100 fewer elements:
<r:code>
n = 32^2
N = as.integer(n - 100L)
</r:code>
We invoke this with 
<r:code>
out = .gpu(cuda.mod$kern, N, ans = integer(N), outputs = "ans", gridDim = 1L, blockDim = c(32^2))
</r:code>
We test the result is as we expect with
<r:test>
stopifnot(identical(out, (1:N) - 1L))
</r:test>
</para>
</section>


<section>
<title>An alternative approach</title>
<para>
We experienced a problem with <lib>NVVM</lib> when converting particular IR code
to PTX. The problem manifested itself as 
<r:error>
Error: R_auto_nvvmCompileProgram NVVM_ERROR_COMPILATION ( NVVM_ERROR_COMPILATION )
(0) Error: unsupported operation
</r:error>
We haven't fully explored the reason for the problem, but by
experimenting with changes in the code, it appears the problem comes
from computing potentially large indices from the thread, block, grid
dimensions and indices to index an array. If we reduce the
computations by removing a multiplicative term in the index, the code
appears to work.  So this lead us to also pursue our original strategy
which is to use <llvm/> to generate the PTX code from the IR code.
</para>

<para>

<llvm/> provides several backends for which we can generate code from
the common IR code.  There are different backends for different
processor types.  There is also a backend for generating the <cpp/>
code that can be used to create the IR code again.  (This is
equivalent to the <r/> code we use to generate the IR.)  Another
backend is NVPTX, or Nividia PTX.  Basically, given an <llvm/>
<r:class>Module</r:class>, there are a series of steps in <llvm/> that
allow us to emit the PTX code for that <r:class>Module</r:class> as a
string. To facilitate switching between  using this approach or <r:func>generatePTX</r:func>
in <omg:pkg>Rnvvm</omg:pkg>, we provide a <r:func>generatePTX</r:func> function
in the <omg:pkg>Rllvm</omg:pkg> package.
Currently, the inputs are slightly different, but this will change in the future
as we integrate and move code in both <omg:pkg>Rnvvm</omg:pkg> and <omg:pkg>RLLVMCompile</omg:pkg>.
</para>

<para>
<lib>NVVM</lib> does some code transformations as it generates the PTX
code for us.  Some of these are necessary to obtain results and so we
have to deal with these when generating the IR code from within <r/>
before we generate the PTX code using only <llvm/>.  The most
important of these relates to parameters which are used to transfer
the results from the kernel back to the caller.  These are
pointers/arrays which the kernel routine modifies,
e.g. <c:arg>out</c:arg> in our kernel.  We must explicitly identify
these as being in the global address space on the device and not local
or shared variables. The multi-level memory system on a GPU is quite
different from the flat memory used by a CPU.  Rather than specifying
the parameter type as, say, <r:var>Int32PtrType</r:var>,
we must create a pointer to the <r:var>Int32Type</r:var>
that is in address space number 1.
We do this with
<r:code>
ty = pointerType(Int32Type, addrspace = 1)
</r:code>
We can work with this as we would a regular pointer type.
However, <llvm/> will generate PTX code that uses
st.global-style operations when assigning to it.
</para>


<para>
The code in <ulink
url="https://github.com/duncantl/Rllvm/blob/master/explorations/ptx_direct_grid.R">explorations/ptx_direct_range.R</ulink>
illustrates how to generate IR code from beginning to end.  We'll
discuss how to generate this code via a higher-level compiler below.
</para>

</section>


<section>
<title>Next steps - High-level compilation</title>

<para>
We clearly don't want to be writing <r/> code to 
create each and every instruction. Instead,
we want to be able to write the code for the kernel 
in a higher-level language and have an <r/> function
compile that to PTX, generating the IR code for the 
different implicit instructions.
We would like to be able to write our kernel as something like
<r:code><![CDATA[
kern =
fnunction(N, out)
{
   idx = blockIdx$x * blockDim$x + threadIdx$x
   if(idx < N)
     out[idx] = idx
}
]]></r:code>
This is simple <r/> code that won't run.
There is no <r:var>blockIdx</r:var> or <r:var>blockDim</r:var>.
However, we can compile it with the <omg:pkg>RLLVMCompile</omg:pkg>
package.
We have implemented a proof-of-concept for compiling
a simple <r/>-like function as a GPU kernel routine.
We can use this with 
<r:code>
globalInt32PtrType = pointerType(Int32Type, addrspace = 1)
fun = compileGPUKernel(kern, list(N = Int32Type, x = globalInt32Type), 
                       .zeroBased = c(idx = TRUE) 
                      )
</r:code>
We then can convert the module to PTX, as above,  with
<r:code>
ptx = generatePTX(fun)
</r:code>
and load it onto the GPU with <r:func>loadModule</r:func>.
</para>


<para>
The compiler recognizes expressions
such as <r:expr eval="false">blockIdx$x</r:expr>
and transforms those to calls to special intrinsic 
functions. 
We also extended the compiler to allow the user
to specify which variables should be treated
as-is for subsetting and not have 1 subtracted from their value.
This is the <r:arg>.zeroBased</r:arg> parameter.
We can also specify the types of local variables rather
than relying on the compiler to use the type of their initial
value. This allows us to 64-bit integers for some computations if we
need.
</para>

<para>
The compilation for GPU code in <r:func>compileGPUKernel</r:func> is
very basic at present. It does illustrate how one can 
customize the basic compilation mechanism and adapt it
to different computational models.
An example is in <ulink url="https://github.com/duncantl/RLLVMCompile/explorations/gpu.R">explorations/gpu.R</ulink>.
</para>
</section>


</article>

