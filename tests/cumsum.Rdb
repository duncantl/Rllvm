<?xml version="1.0"?>
<article xmlns:r="http://www.r-project.org"
         xmlns:c="http://www.C.org"
         xmlns:xi="http://www.w3.org/2003/XInclude"
	 xmlns:sh="http://www.shell.org"
	 xmlns:omg="http://www.omegahat.org">

<articleinfo>

<title>Compiling an <r/> Implementation of the <r:func>cumsum</r:func> Function
</title>

<author><firstname>Duncan</firstname><surname>Temple Lang</surname>
  <affiliation><orgname>University of California at Davis</orgname>
               <orgdiv>Department of Statistics</orgdiv>
  </affiliation>
</author>
</articleinfo>

<section>
<title>The <r/> Code</title>
<para>

Consider the following R function to perform a
cumulative sum on a vector:
<r:code>
Cumsum =
function(x)
{
   for(i in 2:length(x))  # ignore the case where x is of length 1 or 0
     x[i] = x[i-1] + x[i]

   x
}
</r:code>


We can check it agrees with the regular
internally constructed <r:func>cumsum</r:func>
function via
<r:code>
all(Cumsum(1:4) == cumsum(1:4))
</r:code>

</para>
<para>

Now, we want to compile this.
Perhaps a reasonable C version is
<c:code><![CDATA[
void
Cumsum(double *x, double *ans, int len)
{
    int i;
    ans[0] = x[0];
    for(i = 1; i < len; i++)
	ans[i] = ans[i-1] + x[i];
}
]]></c:code>

Note that we are passing in a reference to the original x and a new
vector into which the cumulative sum elements will be inserted.

</para>
</section>

<section id="compileCode">
<title>Compiling the Code</title>
<para>

So we want to write LLVM code to build this routine.

<r:code>
library(Rllvm)

InitializeNativeTarget()
</r:code>

We create a module and declare the function
by specifying the return type and the parameter types.
<r:code>
mod = Module("Cumsum")
fun = Function("Cumsum", VoidType, c(x = DoublePtrType, ans = DoublePtrType, len = Int32Type), mod)
</r:code>
Note that <r:var>DoublePtrType</r:var>
represents a pointer to a <c:type>double</c:type>.
We also named the parameters in the third argument to the call to
<r:func>Function</r:func>, i.e. <r:var>x</r:var>, <r:var>ans</r:var>, <r:var>len</r:var>.
We could do this separately after we call <r:func>Function</r:func> with
<r:code>
names(fun) = c("x", "ans", 'len')
</r:code>


</para>
<para>
Since we want to reference the variables when compiling the code, 
we'll get them as a list of parameter objects in <r/>. (Alternatively, we could
treat the <r:class>Function</r:class> object as a kind
of list and access the parameters via [[, e.g.
<r:expr eval="false">fun[["x"]]</r:expr>.
This isn't implemented and won't always work as the parameters may not have names.)
However,  we'll use the <r:func>getParameters</r:func>
approach for now:
<r:code>
params = getParameters(fun)
</r:code>
</para>

<para>
So now we can start to construct the body of the routine.
We need an entry block in which we will create
and initialize local variables:

<r:code>
entry = Block(fun, "entry")
</r:code>

We'll also create an IRBuilder to manage the construction
of the instructions.
<r:code>
ir = IRBuilder(entry)
</r:code>
By creating this with the <r:var>entry</r:var> block,
new instructions will be created within this block.
When we want to switch to a different block to add instructions, we will
update the variable <r:var>ir</r:var>.

</para>
<para>
We'll need a local variable for the variablei, our loop counter:
<r:code>
iv = ir$createLocalVariable(Int32Type, "i")
</r:code>

We also need local variables to reference the parameters <r:var>x</r:var> and <r:var>y</r:var> and <r:var>len</r:var>
by their address.
<r:code>
xref = ir$createLocalVariable(DoublePtrType, "x_addr")
ans.ref = ir$createLocalVariable(DoublePtrType, "ans_addr")
len.ref = ir$createLocalVariable(Int32Type, "len_addr")
</r:code>

</para>
<para>
Next we initialize these variables.
<q>Can we initialize them in the same order we create them
and combine the allocation and Store?  Seems like yes
based on moving the C++ code to initialize the variables
and compiling and verifying the module</q>

We set <r:var>i</r:var> to be 1:
<r:code>
ir$createStore(1L, iv)
</r:code>

Then we get the address of the parameters
and set the local variables that refer to these.
<r:code>
ir$createStore(params$x, xref)
ir$createStore(params$ans, ans.ref)
ir$createStore(params$len, len.ref)
</r:code>
</para>

<para>
So now we are ready to do some computations.
The first thing is to set <r:expr eval="false">ans[0] = x[0]</r:expr>.
We load <r:expr eval="false">x[0]</r:expr> and then store the value in
<r:expr eval="false">ans[0]</r:expr>, having loaded that.
We load the value of x[0] with
<r:code>
a = ir$createLoad(xref)
b = ir$createGEP(a, 0L)
x0 = ir$createLoad(b)
</r:code>
GEP stands for <quote>Get Element Pointer</quote>.
See <ulink url="http://llvm.org/docs/GetElementPtr.html"/>
</para>

<para>
Next, we assign this to <r:expr eval="false">ans[0]</r:expr>
<r:code>
a = ir$createLoad(ans.ref)
b = ir$createGEP(a, 0L)
ir$createStore(x0, b)
</r:code>
Note the similar sequence access  for both <r:expr eval="false">x[0]</r:expr> and <r:expr eval="false">ans[0]</r:expr>
and the difference in loading and assigning a value to each.
</para>

<para>
At this point we are ready to jump into the loop.  We branch to the
test of the condition and within that we perform the test and
determine which other block to jump to -  the body or the return.  So we
need to create these 3 blocks: the condition test, the body and the
return
<r:code>
cond = Block(fun, "loopCondition")
ret = Block(fun, "return")
body = Block(fun, "loopBody")
</r:code>

Now we branch to the condition, unconditionally.
<r:code>
ir$createBr(cond)
</r:code>
We are now working on that and
so want the new instructions to be
added there. So we tell the <r:class>IRBuilder</r:class> object
<r:var>ir</r:var> to put new instructions there with
<r:code>
ir$setInsertPoint(cond)
</r:code>

To test the condition of the loop iterator, 
we load the values for i and len and then
compare them
<r:code>
a = ir$createLoad(iv)
b = ir$createLoad(len.ref)
ok = ir$createICmp(ICMP_SLT, a, b)
ir$createCondBr(ok, body, ret)
</r:code>
It may help to know that <r:var>ICMP_SLT</r:var> stands for "Integer comparison, Signed Less Than".
</para>

<para>
The return block is very simple. We return nothing
so can create a void return
<r:code>
ir$setInsertPoint(ret)
ir$createRetVoid()
</r:code>
</para>


<para>
So now we focus on the body of the loop: 
<r:code>
ir$setInsertPoint(body)
</r:code>

The loop is also very simple.  We fetch the value of ans[i-1] and
x[i], add them together and store the result in ans[i].  This involves
many low-level steps.  First we load i, then subtract 1 from that
value, then load ans and use the value of i-1 to index the value and
load that.

<r:code>
a = ir$createLoad(iv)  # load i
b = ir$binOp(BinaryOps["Sub"], a, ir$createConstant(1L))  # subtract 1 from i
r = ir$createLoad(ans.ref)  # load ans
idx = ir$createSExt(b, 64L) #  index for ans[i-1]
ans.i1 = ir$createLoad(ir$createGEP(r, idx))
</r:code>
<q>Why can't we just use createGEP with <r:var>b</r:var>?
The SExt is a <quote>signed extension</quote> that transforms the operand to, in our case, 64 bit.</q>
So now we have <r:expr eval="false">ans[i-1]</r:expr>.
</para>
<para>
Next, we have to load the value of x[i]
<r:code>
a = ir$createLoad(xref)
i = ir$createLoad(iv)
idx = ir$createSExt(i, 64L)
xi = ir$createLoad(ir$createGEP(a, idx))
</r:code>
This is the very same idiom as for loading an element of <r:var>ans</r:var> above.
</para>
<para>
Now we can add these two values xi and ans.i1
<r:code>
tmp = ir$binOp(BinaryOps["FAdd"], ans.i1, xi)
</r:code>
Note that we had to know these were two real-valued variables.
</para>
<para>
Now we need to load <r:expr eval="false">ans[i]</r:expr> and
store the value of <r:var>tmp</r:var> in that.
The following gets <r:expr eval="false">ans[i]</r:expr> in place:
<r:code>
x = ir$createLoad(ans.ref)
i = ir$createLoad(iv)
i = ir$createSExt(i, 64L)
ans.i = ir$createGEP(x, i)
</r:code>
Now we store the value from the tmp expression
into ans[i]:
<r:code>
ir$createStore(tmp, ans.i)
</r:code>

</para>
<para>
Now we have performed the body of the loop.
Next, we need to increment i and then 
branch back to cond

<r:code>
i = ir$createLoad(iv)
inc = ir$binOp(BinaryOps["Add"], i, 1L)
ir$createStore(inc, iv)
</r:code>
Now we jump back to the top of the loop:
<r:code>
ir$createBr(cond)
</r:code>
</para>

<para>
<r:code>
showModule(mod)
</r:code>
</para>

</section>

<section>
<title>Calling the Cumsum routine</title>
<para>
Now that we have generated the routine 
we can call it via the <r:func>run</r:func>
command. We would like to be able to just
access it via the .C or .Call interface,
or via Rffi.
<r:code>
x = as.numeric(1:4)
ans = numeric(length(x))
tt = .llvm(fun, x = x, ans = ans, n = length(x), .all = TRUE)
tt$ans
</r:code>
</para>

<para>
Now let's do a quick test of speed.
We'll use the interpreted version
Cumsum, the built-in <r:func>cumsum</r:func>
implemented in C and
our LLVM-generated routine.
<r:code>
N = 1e4
x = as.numeric(1:N)
ans = numeric(length(x))

a = system.time({Cumsum(x)})
b = system.time({cumsum(x)})
c = system.time({ .llvm(fun, x = x, ans = ans, n = length(x), .all = TRUE)$ans})
</r:code>
We should use the measurements from the second and subsequent
invocations of fun to remove any overhead of the
JIT. (These are real and important, but not for
comparing execution speed of the fully optimized function.
They count, along with the time to compile/generate
and optimize the  generated code.)
So we get a speed up of 26 over the interpreted version.
We are significantly slower than the <r:func>cumsum</r:func>.
Is this optimization in the C compiler?
Remember that the built-in <r:func>cumsum</r:func> function is actually doing 
more, dealing with <r:na/> values, etc.
</para>

</section>

<section id="optimization">
<title>Optimization</title>
<para>
<r:code>
ee = ExecutionEngine(mod)
Optimize(mod, ee)
</r:code>
</para>
</section>

<section>
<title>Timings</title>
<para>
<r:code>
N = 10
x = as.numeric(1:N)
ans = numeric(length(x))

val = .llvm(fun, x = x, ans = ans, n = length(x), .all = TRUE, .ee = ee)$ans
all.equal(val, cumsum(x))
</r:code>
</para>

<r:code>
N = 1e7
x = as.numeric(1:N)
ans = numeric(length(x))

interp = replicate(10, system.time({Cumsum(x)}))
native =  replicate(10, system.time({cumsum(x)}))
ll = replicate(10, system.time({.llvm(fun, x = x, ans = ans, n = length(x), .all = TRUE, .ee = ee)$ans}))
print((rowMeans(interp)/rowMeans(ll))[1:3])
</r:code>

<para>
The results on my Mac (under some additional load) give
(these are older results, probably from LLVM 2.7 or 2.8)
<r:output><![CDATA[
 user.self   sys.self    elapsed 
216.012780   6.666667 164.025822 
]]></r:output>

</para>

<para>
Let's compare with the byte-code compiler.
We'll first compare the byte-code compiled run-time with the interpreted
version of the function.
<r:code>
library(compiler)
cCumsum = cmpfun(Cumsum)
compiled = replicate(10, system.time({cCumsum(x)}))
</r:code>
<r:code>
(rowMeans(interp)/rowMeans(compiled))[1:3]
<r:output><![CDATA[
 user.self   sys.self    elapsed 
  3.973048        Inf   3.975758 
]]></r:output>
</r:code>

</para>

<para>
Now we compare the byte-compiled code and the LLVM-compiled code:
<r:code>
(rowMeans(compiled)/rowMeans(ll))[1:3]
<r:output><![CDATA[
 user.self   sys.self    elapsed 
39.8798587  0.3333333 33.6428571 
]]></r:output>
</r:code>
</para>

<para>
Curiously, on eeyore - an Ubuntu Linux machine - 
we get much worse performance comparing 
interpreted to LLVM-compiled code.
The interpreted code is running about 30 faster on 
the Linux box.
The LLVM-code is running about as 10th as fast as it does on OS X.
<r:code>
print((rowMeans(interp)/rowMeans(ll))[1:3], digits = 3)
<r:output><![CDATA[
user.self  sys.self   elapsed
  23.8803    0.0435   20.5082
]]></r:output>
</r:code>
This may be due to  .... ?
</para>


</section>
<section id="Ccode">
<title>C Code</title>
<para>
<sh:code>
R CMD SHLIB cumsum.c
</sh:code>

<r:code>
dyn.load("cumsum.so") # use .Platform$dynlib.ext
</r:code>
<r:code>
cnative =  replicate(10, system.time({.C("Cumsum", x, numeric(length(x)), length(x))}))
</r:code>

<r:code>
cnativeNoDup =  replicate(10, system.time({.C("Cumsum", x, numeric(length(x)), length(x), DUP = FALSE)}))
</r:code>

rbind(
 (rowMeans(cnative)/rowMeans(ll))[1:3],
 (rowMeans(cnativeNoDup)/rowMeans(ll))[1:3],
 (rowMeans(native)/rowMeans(ll))[1:3])
<r:output><![CDATA[
     user.self   sys.self   elapsed
[1,] 0.7922438 0.02631579 0.7196030
[2,] 0.5844875 0.10526316 0.5285360
[3,] 0.5761773 0.00000000 0.5210918
]]></r:output>
</para>
</section>
<section>
<title>Overhead of <r:func>run</r:func></title>

<para>
The numbers below need to be re-run as I was compiling clang and llvm at the same time.
</para>

<para>
How much overhead does the <r:func>run</r:func> function add?
(Can't use this direct approach now as LLVM doesn't provide FFI. We do this ourselves in <omg:pkg>Rllvm</omg:pkg>.)
<r:code eval="false">
ll.rawRun = replicate(10, system.time({ .Call("R_callFunction", fun, .args = list(x = x, ans = ans, n = length(x)), ee)}))
</r:code>

<r:code eval="false">
(rowMeans(ll)/rowMeans(ll.rawRun))[1:3]
<r:output><![CDATA[
user.self  sys.self   elapsed 
 2.675439  7.200000  3.017544 
]]></r:output>
</r:code>
</para>

<para>
What about if we call the function directly, but not via the generic function
<r:code>
ll.runDirect = replicate(10, system.time({ .llvmCallFunction(fun, x = x, ans = ans, n = length(x), .ee = ee) }))
</r:code>

<r:code>
(rowMeans(ll)/rowMeans(ll.runDirect))[1:3]
<r:output><![CDATA[
user.self  sys.self   elapsed 
     2.99       Inf      3.37 
]]></r:output>
</r:code>
So <r:func>.llvmCallFunction</r:func> is  a reasonable
</para>
<para>
So now lets' compare the native <r/> call to 
<r:func>cumsum</r:func> and our direct call to <r:func>fun</r:func>
<r:code>
(rowMeans(native)/rowMeans(ll.runDirect))[1:3]
<r:output><![CDATA[
user.self  sys.self   elapsed 
     2.03       NaN      2.03 
]]></r:output>
</r:code>
(The numbers underlying this are different from the numbers above,
but still done on the same machine.)
This suggests that the LLVM routine we generated is almost twice as fast
</para>


<para>
<r:code>
times = list(interpreted = interp, 
              byteCompiled = compiled,
	      native = native, 
              llvm.run = ll, 
              cCode = cnative,
#              llvm.dotC = ll.rawRun,
              llvm.CallFunction = ll.runDirect)
timings = do.call(rbind, lapply(times, t))
timings = as.data.frame(timings[, 1:3])
src = rep(names(times), sapply(times, ncol))
tm = tapply(timings$elapsed, src, mean)
timings$src = ordered(src, names(rev(sort(tm))))
</r:code>
<r:plot><![CDATA[
library(lattice)
bwplot(elapsed ~ src,  timings)
]]></r:plot>
<r:plot><![CDATA[
bwplot(elapsed ~ src,  timings, subset = !(src %in% c("interpreted", "byteCompiled")))
]]></r:plot>
</para>


<para>
save.image()
</para>

</section>

<section>
<title>Larger Vectors</title>
<para>
Let's try 
<r:code>
N = 1e8
x = as.numeric(1:N)
ans = numeric(N)
native =  replicate(11, system.time({cumsum(x)}))[, 2:10]
ll.runDirect = replicate(11, system.time({ .llvmCallFunction(fun, x = x, ans = ans, n = length(x), .ee = ee) }))[, 2:10]
</r:code>
Note that we run 11 replications, but only take the final 10, i.e. ignore the first one.
This avoids any overhead of the first call.
</para>
<para>
<r:plot><![CDATA[
boxplot(list(llvm = ll.runDirect["elapsed",], native = native["elapsed",]))
]]></r:plot>
</para>


<para>
So the <r:func>.llvm</r:func> function is now an alias for <r:func>.llvmCallFunction</r:func>
and we encourage people to use that.
</para>


</section>
</article>

